from control.decision_gate import decide, Decision
from control.policy_rules import violates_policy
from control.stop_conditions import should_stop
from evaluation.response_validator import validate_response
from audit.logger import log_decision
from audit.decision_record import DecisionRecord


def call_llm_stub(user_query: str) -> dict:
    """
    Placeholder for LLM call.
    This stub simulates a structured model response.
    """
    return {
        "answer": "This is a placeholder response generated by the LLM.",
        "confidence": 0.75
    }


def run(user_query: str) -> str:
    # 1. Decision gate
    decision, reason = decide(user_query)

    log_decision(DecisionRecord(
        user_query=user_query,
        decision=decision.value,
        reason=reason,
        stage="decision_gate"
    ))

    if decision != Decision.ALLOW:
        return f"Stopped at decision gate: {reason}"

    # 2. Policy check
    violated, policy_reason = violates_policy(user_query)

    log_decision(DecisionRecord(
        user_query=user_query,
        decision="refuse" if violated else "allow",
        reason=policy_reason,
        stage="policy_check"
    ))

    if violated:
        return f"Policy refusal: {policy_reason}"

    # 3. LLM call (stub)
    response = call_llm_stub(user_query)

    # 4. Evaluation
    valid, validation_reason = validate_response(response)

    log_decision(DecisionRecord(
        user_query=user_query,
        decision="accept" if valid else "reject",
        reason=validation_reason,
        stage="evaluation",
        confidence=response.get("confidence")
    ))

    if not valid:
        # 5. Stop condition
        if should_stop(attempt_count=1, max_attempts=1, uncertainty_score=1.0):
            return f"Stopped after evaluation: {validation_reason}"

    return response["answer"]


if __name__ == "__main__":
    user_input = input("Enter your query: ")
    output = run(user_input)
    print(output)
